<!doctype html public "-//W3C//DTD HTML 4.0 Transitional //EN">
<html>
<head>
  <meta name="GENERATOR" content="mkd2html 2.2.6 GITHUB_CHECKBOX">
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
  <title></title>

<style>
    table, th, td {
    border: 1px solid black;
    }
    table.center {
    margin-left: auto;
    margin-right: auto;
    }
</style>
</head>
<body>
<h1>2D Object Recognition</h1>

<h3>Greg Attra</h3>

<h3>CS 5330</h3>

<h2>Overview</h2>

<p>The OR2D program is a real-time 2D object recognition program which uses thresholding to isolate regions of an image, segmentation to label those regions, and computes moments on those regions to extract translation and rotation invariant features about the object. Once those features have been aquired, they are labeled and saved to be used for classification.</p>

<h2>Threshold</h2>

<p>This is the first step in the object recognition pipeline. With the program running, press <code>t</code> to threshold the image. The user will be prompted enter the intensity value with which to threshold the image. The result will be a binary image with the background in black and the object in white.</p>

<p>As a final step, the system will perform morphological operations to clean up the threshold image. Specifically, it will <code>dialate</code> and then <code>erode</code> the threshold image. This means it is best to select a threshold value on the lower side so as to minimize the chance of island regions. Dialation will fill most gaps in regions that represent the same object which may result from the lower threshold value.</p>

<p><strong>Examples:</strong>
<br>
<img width=150px src="images/examples/threshold/glove_threshold.png">
<img width=150px src="images/examples/threshold/headphone_threshold.png">
<img width=150px src="images/examples/threshold/shoe_threshold.png">
<img width=150px src="images/examples/threshold/multi_threshold.png"></p>

<h2>Segment</h2>

<p>This step succeeds the threshold step and labels each region with an id, coloring the regions as well. To segment an image, press <code>s</code>. The user will be prompted to enter the max number of regions to segment. While this step does not discard regions that are considered too small to constitute an object for recognition (<code>&lt; MAX_REGION_PIXELS</code>), the <code>Feature</code> step will ignore small regions when computing features and so they will not be labeled or classified by the system.</p>

<p><strong>Examples:</strong>
<br>
<img width=150px src="images/examples/segment/glove_segment.png">
<img width=150px src="images/examples/segment/multi_segment.png">
<img width=150px src="images/examples/segment/shoe_segment.png"></p>

<h2>Features</h2>

<p>This step follows the segment step and computes translation and rotation invariant features for each region in the segmented image. These features, such as the oriented bounding box, are drawn on the image and displayed. Press <code>f</code> to compute features on the image.</p>

<p><img width=350px src="images/examples/features/multi_features.png"></p>

<h2>Label</h2>

<p>This step takes the features computed a region and labels them, storing these features in a file in the <code>labels/</code> directory. Each file can store multiple samples. To initiate this step, press <code>l</code>. The user will be prompted to enter the label of the object. This will capture one feature sample and save it to the data file. To capture another, simply press <code>l</code> again and repeat the process.</p>

<p><strong>Note</strong>: this step assumes that only one object is present in the image. If there are multiple objects, whichever object is labeled as region 1 will be labeled.</p>

<h2>Classify</h2>

<p>This step classifies an image by computing its features and finding the closest matching feature set from the datafiles in the <code>labels/</code> directory. Press <code>c</code> to classify objects. It uses the following equation to compute the distances (where <code>Feature_A</code> is a feature from the image to classify and <code>Feature_B</code> is that same feature from a labeled sample in the database):
<code>
float distance = 1 - (min(feature_A, feature_B) / max(feature_A, feature_B))
</code></p>

<h3>KNN Classify</h3>

<p>There is also the ability to classify objects using a K-nearest neighbors classifier. Press <code>k</code> to classify objects using this algorithm. The user will be prompted to specify the value for <code>K</code> (the number of numbers to sample for each label).</p>

<p><strong>Examples:</strong>
<br>
<img width=300px src="images/examples/classification/headphone_classify.png">
<img width=300px src="images/examples/classification/shoe_classify.png">
<img width=300px src="images/examples/classification/phone_classify.png">
<img width=300px src="images/examples/classification/coaster_classify.png">
<img width=300px src="images/examples/classification/controller_classify.png">
<img width=300px src="images/examples/classification/flipflop_classify.png">
<img width=300px src="images/examples/classification/glove_classify.png">
<img width=300px src="images/examples/classification/hat_classify.png">
<img width=300px src="images/examples/classification/speaker_classify.png">
<img width=300px src="images/examples/classification/remote_classify.png"></p>

<h2>Confusion Matrix</h2>

<p>To evaluate the performance of the system, I ran 5 classifications for each of the 10 objects known to the system. I also ran classifications for new objects the system had not yet seen, but were similar to known objects. For example, while the system was given features for one set of headphones, I ran classification on a different set of headphones which looked similar.</p>

<p>Overall the system performed well. Misclassifications only took place when the object was partially out of the camera frame or there were significant shadows present. The one exception to that was the glove, which had a tendancy to be misclassified as a flipflop even when well lit and within the frame.</p>

<p><strong>Objects</strong>
<br>
<img width=150px src="images/examples/original/coaster_original.png">
<img width=150px src="images/examples/original/controller_original.png">
<img width=150px src="images/examples/original/flipflop_original.png">
<img width=150px src="images/examples/original/glove_original.png">
<img width=150px src="images/examples/original/hat_original.png">
<img width=150px src="images/examples/original/headphone_original.png">
<img width=150px src="images/examples/original/phone_original.png">
<img width=150px src="images/examples/original/remote_original.png">
<img width=150px src="images/examples/original/shoe_original.png">
<img width=150px src="images/examples/original/speaker_original.png">
<br>
<em>Unlabeled Objects</em>
<br>
<img width=150px src="images/examples/original/alt_headphones_original.png">
<img width=150px src="images/examples/original/plantpot_original.png">
<img width=150px src="images/examples/original/brace_original.png"></p>

<p><strong>Confusion Matrix:</strong></p>

<p>(Actual on the top row, Prediction along the side)</p>





<table>
    <tr>
        <th><strong>Actual ⬇ | Predicted ➡ </strong></th>
        <th><strong>Coaster</strong></th>
        <th><strong>Controller</strong></th>
        <th><strong>Flipflop</strong></th>
        <th><strong>Glove</strong></th>
        <th><strong>Hat</strong></th>
        <th><strong>Headphone</strong></th>
        <th><strong>Phone</strong></th>
        <th><strong>Remote</strong></th>
        <th><strong>Shoe</strong></th>
        <th><strong>Speaker</strong></th>
    </tr>
    <tr>
        <th><strong>Coaster</strong></td>
        <td>5</td>
        <td>0</td>
        <td>0</td>
        <td>0</td>
        <td>0</td>
        <td>0</td>
        <td>0</td>
        <td>0</td>
        <td>0</td>
        <td>0</td>
    </tr>
    <tr>
        <th><strong>Controller</strong></td>
        <td>0</td>
        <td>4</td>
        <td>0</td>
        <td>0</td>
        <td>0</td>
        <td>0</td>
        <td>0</td>
        <td>0</td>
        <td>1</td>
        <td>0</td>
    </tr>
    <tr>
        <th><strong>Flipflop</strong></td>
        <td>0</td>
        <td>0</td>
        <td>4</td>
        <td>0</td>
        <td>0</td>
        <td>0</td>
        <td>0</td>
        <td>1</td>
        <td>0</td>
        <td>0</td>
    </tr>
    <tr>
        <th><strong>Glove</strong></td>
        <td>0</td>
        <td>0</td>
        <td>2</td>
        <td>3</td>
        <td>0</td>
        <td>0</td>
        <td>0</td>
        <td>0</td>
        <td>0</td>
        <td>0</td>
    </tr>
    <tr>
        <th><strong>Hat</strong></td>
        <td>0</td>
        <td>0</td>
        <td>1</td>
        <td>0</td>
        <td>4</td>
        <td>0</td>
        <td>0</td>
        <td>0</td>
        <td>0</td>
        <td>0</td>
    </tr>
    <tr>
        <th><strong>Headphone</strong></td>
        <td>0</td>
        <td>0</td>
        <td>0</td>
        <td>0</td>
        <td>0</td>
        <td>5</td>
        <td>0</td>
        <td>0</td>
        <td>0</td>
        <td>0</td>
    </tr>
    <tr>
        <th><strong>Phone</strong></td>
        <td>0</td>
        <td>0</td>
        <td>0</td>
        <td>0</td>
        <td>0</td>
        <td>0</td>
        <td>4</td>
        <td>1</td>
        <td>0</td>
        <td>0</td>
    </tr>
    <tr>
        <th><strong>Remote</strong></td>
        <td>0</td>
        <td>0</td>
        <td>0</td>
        <td>0</td>
        <td>0</td>
        <td>0</td>
        <td>0</td>
        <td>5</td>
        <td>0</td>
        <td>0</td>
    </tr>
    <tr>
        <th><strong>Shoe</strong></td>
        <td>0</td>
        <td>0</td>
        <td>0</td>
        <td>0</td>
        <td>0</td>
        <td>0</td>
        <td>0</td>
        <td>0</td>
        <td>4</td>
        <td>1</td>
    </tr>
    <tr>
        <th><strong>Speaker</strong></td>
        <td>0</td>
        <td>0</td>
        <td>0</td>
        <td>0</td>
        <td>0</td>
        <td>0</td>
        <td>0</td>
        <td>2</td>
        <td>0</td>
        <td>3</td>
    </tr>
    <tr>
        <td>UNLABELED OBJECTS:</td>
    </tr>
    <tr>
        <th><strong>Alt. Headphones</strong></td>
        <td>0</td>
        <td>0</td>
        <td>0</td>
        <td>0</td>
        <td>0</td>
        <td>5</td>
        <td>0</td>
        <td>0</td>
        <td>0</td>
        <td>0</td>
    </tr>
    <tr>
        <th><strong>Plant Pot</strong></td>
        <td>5</td>
        <td>0</td>
        <td>0</td>
        <td>0</td>
        <td>0</td>
        <td>0</td>
        <td>0</td>
        <td>0</td>
        <td>0</td>
        <td>0</td>
    </tr>
    <tr>
        <th><strong>Metal Brace</strong></td>
        <td>0</td>
        <td>0</td>
        <td>0</td>
        <td>0</td>
        <td>0</td>
        <td>0</td>
        <td>2</td>
        <td>3</td>
        <td>0</td>
        <td>0</td>
    </tr>
</table>


<p><br></p>

<h2>Video Demo</h2>

<p>Here is a demo of the system in action: https://youtu.be/iPKZYy79JhM</p>

<h2>Extensions</h2>

<p><strong>2-Pass Segmentation</strong></p>

<p>A user can choose to segment the image using the 2-pass segmentation algorithm instead of OpenCV&rsquo;s
built in <code>connectedComponents()</code> function. Press <code>u</code> to segment an image using this algorithm. To
see the code implementation, see <code>twoPassSegment.cpp</code>.</p>

<p><strong>Multi-object Recognition</strong></p>

<p>As the video shows, the system can classify up to N objects simultaneously, where N is the number of regions specified by the user. To provide this value, press <code>d</code> and a prompt will request the max number of regions to segment.</p>

<p><strong>Extra Feature Detection/Display</strong></p>

<p>For each region, the system computes the bounding box and oriented bounding box, displaying each in the output image, along with the height and width of the oriented bounding box.</p>

<h2>Reflection</h2>

<p>Overall, I found this project very rewarding. The program is relatively fast, in that it can process
video feed in real-time as objects are introduced and removed. After struggling with the computation
for the moments about the central axis and getting the right corners for the oriented bounding box,
it was incredibly satisfactory to see it working in action. I&rsquo;m also happy with my code design.
There are always ways I could have improved, but being new to C++ at the start of this course, I
am picking up on it relatively quickly. This project I leveraged OOP extensively to minimize code
redundancy. Additionally, the nested nature of the pipeline objects makes it easy to create pipelines
which terminate at various steps without needing to change the way the controller behaves.</p>

<p>I was also surprised at how well the system was able to classify objects, especially given such
limitted feature data (height, width, % oriented bounding box filled, and one moment about the central
axis). Of course, this can be attributed to the fact that it was only trained on 10 objects which
themselves were very diverse is size and shape. Plus, given the blank white backdrop created a very
easy environment for segmentation.</p>

<h2>Resources</h2>

<ul>
<li>OpenCV Documentation</li>
<li>TutorialsPoint</li>
</ul>

</body>
</html>
